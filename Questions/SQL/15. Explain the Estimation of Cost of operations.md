## 15. Explain the Estimation of Cost of operations

Estimation of cost of operations is a vital part of **query optimization** and the generation of **equivalence plans.** When generating query evaluation plans, the evaluation plan ultimately chosen is the one that is deemed to have **lowest estimated cost.** The actual cost of the evaluation plan is not known beforehand, but the DBMS has statistics for estimating the cost of the operations put into an evaluation plan.

Query evaluation is made up of studies of the different individual operations that can make up an evaluation plan. The operations can be categorized by their **CPU time, disk accesses, cost of communication** and the like. Disk access speeds are usually the most important variable, due to the currently still relatively slow speeds of read/write
access to disk. 

For example, one of the more important factors taken into consideration when estimating the cost of an evaluation plan is **number of block transfers** and **number of disk seeks.** This means **copying from memory to memory** and **searching for data on the storage disk.** **Block writes** are a further detailing of the memory operations that should also be taken into consideration, as it is approximately twice as resource-intensive as a **block transfer.**

When calculating which possible expressions are the most optimal, the DBMS uses both **equivalence rules** and **catalogued information** about the database to make a decision. The DBMS catalogues statistical information about the database itself to make these guesses; specifically the following:

**n­­r** – The number of tuples in the relation r.

**br** – The number of blocks/memory containing tuples of relation r.

**lr** – The size of a tuple of relation r in bytes.

**fr** – The blocking factor of relation r, which is the number of tuples of relation r that fit into one block.

**V(A,r)** – The number of distinct values appearing in the relation r for attribute A.

These statistics are usually updated by the DBMS during low load. Preferably it would be able to update it on every modification, but due to performance issues it’s restricted to low-load occasions where it can partition the processing power required for it without slowing down the rest of the system.

DBMS systems also typically store **histograms** of the values stored in the database, plotting values into groupings in a histogram to analyze the frequency of certain value ranges in order to take this into account.

Cost-based optimization does have a drawback, in that the optimization itself is, naturally, costly in itself. The cost of the optimization can be reduced through **heuristics**, meaning practical methods that may not be guaranteed optimal, but sufficiently efficient for the given task.

An example of heuristics in cost-based optimization is heuristic rules pertaining to relational algebra; for example, "*Perform selection operations as early as possible.*" Having this **heuristic rule** means the compiler will follow this rule without measuring whether or not it will be the optimal way to go. The reason for this is that while there will be occasions where it is *not* optimal, the majority of the time, the rule is seen as sufficiently optimal, to the point where applying the rule without analyzing how optimal it is will result in a net positive.

Another example of a **heuristic rule** is: "*Perform projections early.*" This is seen as an efficient heuristic rule because **projections** reduce the size of relations, which in turn reduces the amount of block transfers required for the query.

